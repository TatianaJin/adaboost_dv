{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from numpy.core.umath_tests import inner1d\n",
    "\n",
    "from six import with_metaclass\n",
    "from six.moves import xrange\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import BaseEnsemble\n",
    "from sklearn.ensemble.forest import BaseForest\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree.tree import BaseDecisionTree\n",
    "from sklearn.tree._tree import DTYPE\n",
    "from sklearn.utils.validation import check_array, check_X_y, check_random_state, has_fit_parameter, check_is_fitted\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseWeightBoosting(with_metaclass(ABCMeta, BaseEnsemble)):\n",
    "    \"\"\"Base class for Adaboost classifier\n",
    "\n",
    "    class BaseWeightBoosting inherits BaseEnsemble\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    learning_rate, random_state, estimator_weights_, estimator_errors_\n",
    "    \"\"\"\n",
    "    \n",
    "    # the same as in sklearn.ensemble.BaseWeightBoosting\n",
    "    @abstractmethod\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 estimator_params=tuple(),\n",
    "                 learning_rate=1.,\n",
    "                 random_state=None):\n",
    "        super(BaseWeightBoosting, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            estimator_params=estimator_params\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state      \n",
    "\n",
    "    def _clear(self):\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _boost(self, iboost, X, y, sample_weight, random_state):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Build a boosted classifier from the training set (X, y).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is forced to DTYPE from tree._tree if the base classifier of this\n",
    "            ensemble weighted boosting classifier is a tree or forest.\n",
    "\n",
    "        y: array-like of shape = [n_samples]\n",
    "            The target values (class labels).\n",
    "\n",
    "        sample_weight : array-like of shape = [n_samples], optional\n",
    "            Sample weights. If None, the sample weights are initialized to\n",
    "            1 / n_samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        ##### CHECK PARAMETERS (Support classification only) #####\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "            \n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                 BaseForest))):\n",
    "            dtype = DTYPE\n",
    "            accept_sparse = 'csc'\n",
    "        else:\n",
    "            dtype = None\n",
    "            accept_sparse = ['csr', 'csc']\n",
    "            \n",
    "        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype, y_numeric=False)\n",
    "        \n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "            sample_weight.fill(1. / X.shape[0])\n",
    "        else:\n",
    "            sample_weight = check_array(sample_weight, accept_sparse=False, ensure_2d=False)\n",
    "            # Normalize existing weights\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "            # Check that the sample weights sum is positive\n",
    "            if sample_weight.sum() <= 0:\n",
    "                raise ValueError(\n",
    "                    \"Attempting to fit with a non-positive \"\n",
    "                    \"weighted number of samples.\")\n",
    "                \n",
    "        self._validate_estimator()\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        ########## END OF CHECK PARAMETERS ##########\n",
    "        \n",
    "        # Clear previous states\n",
    "        self._clear()\n",
    "        \n",
    "        ###### BOOSTING #####\n",
    "        for iboost in xrange(self.n_estimators):\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                iboost,\n",
    "                X, y,\n",
    "                sample_weight,\n",
    "                random_state)\n",
    "\n",
    "            # Early termination\n",
    "            if sample_weight is None: # None when the base classifier is worse than random\n",
    "                break\n",
    "                \n",
    "            # estimator_errors seem not be used but only recorded\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Early termination\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "                \n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize\n",
    "                sample_weight /= sample_weight_sum\n",
    "        ###### END OF BOOSTING #####\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def staged_score(self, X, y, sample_weight=None):\n",
    "        for y_pred in self.staged_predict(X):\n",
    "            yield accuracy_score(y, y_pred, sample_weight=sample_weight)\n",
    "            \n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        \"\"\"Return the feature importances (the higher, the more important the\n",
    "           feature).\n",
    "        Returns\n",
    "        -------\n",
    "        feature_importances_ : array, shape = [n_features]\n",
    "        \"\"\"\n",
    "        if self.estimators_ is None or len(self.estimators_) == 0:\n",
    "            raise ValueError(\"Estimator not fitted, \"\n",
    "                             \"call `fit` before `feature_importances_`.\")\n",
    "\n",
    "        try:\n",
    "            norm = self.estimator_weights_.sum()\n",
    "            return (sum(weight * clf.feature_importances_ for weight, clf\n",
    "                    in zip(self.estimator_weights_, self.estimators_))\n",
    "                    / norm)\n",
    "\n",
    "        except AttributeError:\n",
    "            raise AttributeError(\n",
    "                \"Unable to compute feature importances \"\n",
    "                \"since base_estimator does not have a \"\n",
    "                \"feature_importances_ attribute\")\n",
    "            \n",
    "    def _validate_X_predict(self, X):\n",
    "        \"\"\"Ensure that X is in the proper format\"\"\"\n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator,\n",
    "                           (BaseDecisionTree, BaseForest))):\n",
    "            X = check_array(X, accept_sparse='csr', dtype=DTYPE)\n",
    "\n",
    "        else:\n",
    "            X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _samme_proba(estimator, n_classes, X):\n",
    "    \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X)\n",
    "\n",
    "    # Displace zero probabilities so the log is defined.\n",
    "    # Also fix negative elements which may occur with\n",
    "    # negative sample weights.\n",
    "    proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps\n",
    "    log_proba = np.log(proba)\n",
    "\n",
    "    return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                              * log_proba.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Attributes (self)\n",
    "    ----------\n",
    "    algorithm:   the adaboost algorithm to use\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1,\n",
    "                 algorithm='BINARY', # default original adaboost\n",
    "                 random_state=None,\n",
    "                 dv_interval=50,\n",
    "                 dv_max_iter=50,\n",
    "                 dv_loss=\"log\",\n",
    "                 dv_penalty=\"none\"):\n",
    "        # If algorithm is DYNAMIC_VOTES,\n",
    "        # the default is to adjust votes only at the end of training\n",
    "        super(AdaBoostClassifier, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state)\n",
    "        self.algorithm = algorithm\n",
    "        self.dv_interval = dv_interval\n",
    "        self.dv_max_iter=dv_max_iter\n",
    "        self.dv_loss=dv_loss\n",
    "        self.dv_penalty=dv_penalty\n",
    "        \n",
    "    def fit_test(self, X, y, X_test, y_test):\n",
    "        \"\"\" test after each iteration for monitoring \"\"\"\n",
    "        \n",
    "        start = time()\n",
    "        testing_time = 0\n",
    "        \n",
    "        # Initialize weights to 1 / n_samples\n",
    "        sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "        sample_weight.fill(1. / X.shape[0])\n",
    "        \n",
    "        ##### CHECK PARAMETERS (Support classification only) #####\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "            \n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                 BaseForest))):\n",
    "            dtype = DTYPE\n",
    "            accept_sparse = 'csc'\n",
    "        else:\n",
    "            dtype = None\n",
    "            accept_sparse = ['csr', 'csc']\n",
    "            \n",
    "        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype, y_numeric=False)\n",
    "        X_test, y_test = check_X_y(X_test, y_test, accept_sparse=accept_sparse, dtype=dtype, y_numeric=False)\n",
    "        \n",
    "        self._validate_estimator()\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        ########## END OF CHECK PARAMETERS ##########\n",
    "        \n",
    "        # Clear previous states\n",
    "        self._clear()\n",
    "        \n",
    "        ###### BOOSTING #####\n",
    "        for iboost in xrange(self.n_estimators):\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                iboost,\n",
    "                X, y,\n",
    "                sample_weight,\n",
    "                random_state)\n",
    "\n",
    "            # Early termination\n",
    "            if sample_weight is None: # None when the base classifier is worse than random\n",
    "                break\n",
    "                \n",
    "            # estimator_errors seem not be used but only recorded\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            start_testing = time()\n",
    "            # test on testing data\n",
    "            error = (self.predict(X_test) != y_test).sum() / len(y_test)\n",
    "            print(\"[Iter %s] error=%s\" % (iboost, error))\n",
    "            testing_time += time() - start_testing\n",
    "            \n",
    "            # Early termination\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "                \n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize\n",
    "                sample_weight /= sample_weight_sum\n",
    "        ###### END OF BOOSTING #####\n",
    "        \n",
    "        training_time = time() - start - testing_time\n",
    "        print(\"fitting time: %ss\" % training_time)\n",
    "        return self\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # Check that the algorithm is supported\n",
    "        if self.algorithm not in ('SAMME.R', 'SAMME',\n",
    "                                  'DYNAMIC_VOTES', 'BINARY'):\n",
    "            raise ValueError(\"algorithm %s is not supported\"\n",
    "                             % self.algorithm)\n",
    "        \n",
    "        # Fit\n",
    "        return super(AdaBoostClassifier, self).fit(X, y, sample_weight)\n",
    "    \n",
    "    def _validate_estimator(self):\n",
    "        super(AdaBoostClassifier, self)._validate_estimator(\n",
    "            default=DecisionTreeClassifier(max_depth=1))\n",
    "        \n",
    "        # SAMME-R requires predict_proba-enabled base estimators\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            if not hasattr(self.base_estimator_, 'predict_proba'):\n",
    "                raise TypeError(\n",
    "                    \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\n",
    "                    \"that the weak learner supports the calculation of class \"\n",
    "                    \"probabilities with a predict_proba method.\\n\"\n",
    "                    \"Please change the base estimator or set \"\n",
    "                    \"algorithm='SAMME' instead.\")\n",
    "        if not has_fit_parameter(self.base_estimator_, \"sample_weight\"):\n",
    "            raise ValueError(\"%s doesn't support sample_weight.\"\n",
    "                             % self.base_estimator_.__class__.__name__)\n",
    "        \n",
    "        if self.algorithm == \"DYNAMIC_VOTES\":\n",
    "            # Validate dv_interval\n",
    "            if self.dv_interval <= 0:\n",
    "                raise ValueError(\"dv_interval must be postive\")\n",
    "            if self.dv_loss not in (\"log\", \"hinge\", \"perceptron\"):\n",
    "                raise ValueError(\"dv_loss can only be log, hinge, or perceptron\")\n",
    "            if self.dv_penalty not in (\"none\", \"l1\", \"l2\"):\n",
    "                raise ValueError(\"dv_penalty: %s is not supported\" % self.dv_penalty)\n",
    "\n",
    "    def _boost(self, iboost, X, y, sample_weight, random_state):\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            return self._boost_real(iboost, X, y, sample_weight,\n",
    "                                    random_state)\n",
    "\n",
    "        elif self.algorithm == 'BINARY':\n",
    "            return self._boost_binary(iboost, X, y, sample_weight,\n",
    "                                        random_state)\n",
    "        \n",
    "        elif self.algorithm == 'DYNAMIC_VOTES':\n",
    "            return self._boost_dv(iboost, X, y, sample_weight,\n",
    "                                        random_state)\n",
    "        \n",
    "        else:  # elif self.algorithm == \"SAMME\":\n",
    "            return self._boost_discrete(iboost, X, y, sample_weight,\n",
    "                                        random_state)\n",
    "    \n",
    "    def _boost_real(self, iboost, X, y, sample_weight, random_state):\n",
    "        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n",
    "        estimator = self._make_estimator(random_state=random_state)\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "        \n",
    "        # Predict class probabilities\n",
    "        y_predict_proba = estimator.predict_proba(X)\n",
    "        \n",
    "        if iboost == 0:\n",
    "            self.classes_ = getattr(estimator, 'classes_', None)\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            \n",
    "        # Get the class with largest probability\n",
    "        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),\n",
    "                                       axis=0)\n",
    "        \n",
    "        # Instances incorrectly classified\n",
    "        incorrect = y_predict != y\n",
    "        \n",
    "        # Error fraction\n",
    "        estimator_error = np.mean(\n",
    "            np.average(incorrect, weights=sample_weight, axis=0))\n",
    "        \n",
    "        # Stop if classification is perfect\n",
    "        if estimator_error <= 0:\n",
    "            return sample_weight, 1., 0.\n",
    "        \n",
    "        # Construct y coding as described in Zhu et al [2]:\n",
    "        #\n",
    "        #    y_k = 1 if c == k else -1 / (K - 1)\n",
    "        #\n",
    "        # where K == n_classes_ and c, k in [0, K) are indices along the second\n",
    "        # axis of the y coding with c being the index corresponding to the true\n",
    "        # class label.\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_\n",
    "        y_codes = np.array([-1. / (n_classes - 1), 1.])\n",
    "        y_coding = y_codes.take(classes == y[:, np.newaxis])\n",
    "    \n",
    "        # Displace zero probabilities so the log is defined.\n",
    "        # Also fix negative elements which may occur with\n",
    "        # negative sample weights.\n",
    "        proba = y_predict_proba  # alias for readability\n",
    "        proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps\n",
    "\n",
    "        # Boost weight using multi-class AdaBoost SAMME.R alg\n",
    "        estimator_weight = (-1. * self.learning_rate\n",
    "                                * (((n_classes - 1.) / n_classes) *\n",
    "                                   inner1d(y_coding, np.log(y_predict_proba))))\n",
    "\n",
    "        # Only boost the weights if it will fit again\n",
    "        if not iboost == self.n_estimators - 1:\n",
    "            # Only boost positive weights\n",
    "            sample_weight *= np.exp(estimator_weight *\n",
    "                                    ((sample_weight > 0) |\n",
    "                                     (estimator_weight < 0)))\n",
    "\n",
    "        return sample_weight, 1., estimator_error\n",
    "\n",
    "    def _boost_discrete(self, iboost, X, y, sample_weight, random_state):\n",
    "        \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n",
    "        estimator = self._make_estimator(random_state=random_state)\n",
    "\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        y_predict = estimator.predict(X)\n",
    "\n",
    "        if iboost == 0:\n",
    "            self.classes_ = getattr(estimator, 'classes_', None)\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "\n",
    "        # Instances incorrectly classified\n",
    "        incorrect = y_predict != y\n",
    "\n",
    "        # Error fraction\n",
    "        estimator_error = np.mean(\n",
    "            np.average(incorrect, weights=sample_weight, axis=0))\n",
    "\n",
    "        # Stop if classification is perfect\n",
    "        if estimator_error <= 0:\n",
    "            return sample_weight, 1., 0.\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "\n",
    "        # Stop if the error is at least as bad as random guessing\n",
    "        if estimator_error >= 1. - (1. / n_classes):\n",
    "            self.estimators_.pop(-1)\n",
    "            if len(self.estimators_) == 0:\n",
    "                raise ValueError('BaseClassifier in AdaBoostClassifier '\n",
    "                                 'ensemble is worse than random, ensemble '\n",
    "                                 'can not be fit.')\n",
    "            return None, None, None\n",
    "\n",
    "        # Boost weight using multi-class AdaBoost SAMME alg\n",
    "        estimator_weight = self.learning_rate * (\n",
    "            np.log((1. - estimator_error) / estimator_error) +\n",
    "            np.log(n_classes - 1.))\n",
    "\n",
    "        # Only boost the weights if I will fit again\n",
    "        if not iboost == self.n_estimators - 1:\n",
    "            # Only boost positive weights\n",
    "            sample_weight *= np.exp(estimator_weight * incorrect *\n",
    "                                    ((sample_weight > 0) |\n",
    "                                     (estimator_weight < 0)))\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "    \n",
    "    def _boost_binary(self, iboost, X, y, sample_weight, random_state, keep_predict=False):\n",
    "        \"\"\"Implement a single boost for binary classification.\"\"\"\n",
    "        estimator = self._make_estimator(random_state=random_state)\n",
    "\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        y_predict = estimator.predict(X)\n",
    "\n",
    "        if keep_predict:\n",
    "            if iboost == 0:\n",
    "                self.predicts_ = np.array([y_predict])\n",
    "            else:\n",
    "                self.predicts_ = np.append(self.predicts_, np.array([y_predict]), axis=0)\n",
    "\n",
    "        if iboost == 0:\n",
    "            self.classes_ = getattr(estimator, 'classes_', None)\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            if self.n_classes_ is not 2:\n",
    "                raise ValueError(\"The binary classification algorithm\"\n",
    "                                 \"only supports two classes\")\n",
    "\n",
    "        # Instances incorrectly classified\n",
    "        incorrect = y_predict != y\n",
    "\n",
    "        # Error fraction\n",
    "        estimator_error = np.average(\n",
    "            incorrect, weights=sample_weight, axis=0)\n",
    "\n",
    "        # Stop if classification is perfect\n",
    "        if estimator_error <= 0:\n",
    "            return sample_weight, 1., 0.\n",
    "\n",
    "        # Convert to 1 / -1 from 1 / 0\n",
    "        incorrect = incorrect * 2 - 1\n",
    "\n",
    "        # Stop if the error is as bad as random guessing\n",
    "        if estimator_error == 0.5:\n",
    "            self.estimators_.pop(-1)\n",
    "            if len(self.estimators_) == 0:\n",
    "                raise ValueError('BaseClassifier in AdaBoostClassifier '\n",
    "                                 'ensemble is as random, ensemble '\n",
    "                                 'can not be fit.')\n",
    "            return None, None, None\n",
    "\n",
    "        # Boost weight using multi-class AdaBoost SAMME alg\n",
    "        estimator_weight = 0.5 * (\n",
    "            np.log((1. - estimator_error) / estimator_error))\n",
    "\n",
    "        # Only boost the weights if I will fit again\n",
    "        if not iboost == self.n_estimators - 1:\n",
    "            # Only boost positive weights\n",
    "            sample_weight *= np.exp(estimator_weight * incorrect *\n",
    "                                    ((sample_weight > 0) |\n",
    "                                     (estimator_weight < 0)))\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "    \n",
    "    def _boost_dv(self, iboost, X, y, sample_weight, random_state):\n",
    "        \"\"\"Implement a single boost adjusting all votes every\n",
    "        dv_interval iterations\"\"\"\n",
    "        sample_weight, estimator_weight, estimator_error = self._boost_binary(\n",
    "            iboost, X, y, sample_weight, random_state, keep_predict=True)\n",
    "        \n",
    "        if (iboost+1) % self.dv_interval == 0: # adjust the votes\n",
    "            print(\"[Iter %s] Dynamically adjusting votes\" % iboost)\n",
    "            \n",
    "            X_map = self.predicts_.T\n",
    "            \n",
    "            # 1. Train the votes\n",
    "            vote_model = SGDClassifier(\n",
    "                loss=self.dv_loss,\n",
    "                penalty=self.dv_penalty,\n",
    "                fit_intercept=False,\n",
    "                max_iter=self.dv_max_iter)\n",
    "            \n",
    "            coef_init = np.array([np.append(self.estimator_weights_[:iboost], estimator_weight)])\n",
    "            # Assume equal initial sample weights for simplicity\n",
    "            vote_model.fit(X_map, y, coef_init=coef_init)\n",
    "\n",
    "            # Normalize the votes\n",
    "            self.estimator_weights_[:iboost+1] = vote_model.coef_[0] / vote_model.coef_[0].sum() * coef_init.sum()\n",
    "            estimator_weight = self.estimator_weights_[iboost]\n",
    "            print(\"TODO !!! try: normalize, not normalize (now keeping constant sum)\")\n",
    "            \n",
    "            # Calculate sample weights (assume equal initial sample weights)\n",
    "            sample_weight = np.exp(X_map.dot(self.estimator_weights_[:iboost+1]) * y * -1)\n",
    "            sample_weight /= sample_weight.sum()\n",
    "        \n",
    "        # NOTICE: the estimator error reports error irresponsive to dynamic votes\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute the decision function of ``X``.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "        Returns\n",
    "        -------\n",
    "        score : array, shape = [n_samples, k]\n",
    "            The decision function of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "            Binary classification is a special cases with ``k == 1``,\n",
    "            otherwise ``k==n_classes``. For binary classification,\n",
    "            values closer to -1 or 1 mean more like the first or second\n",
    "            class in ``classes_``, respectively.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"n_classes_\")\n",
    "        X = self._validate_X_predict(X)\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            pred = sum(_samme_proba(estimator, n_classes, X)\n",
    "                       for estimator in self.estimators_)\n",
    "        else: # self.algorithm in (\"SAMME\", \"BINARY\", \"DYNAMIC_VOTES\")\n",
    "            pred = sum((estimator.predict(X) == classes).T * w\n",
    "                       for estimator, w in zip(self.estimators_,\n",
    "                                               self.estimator_weights_))\n",
    "\n",
    "        pred /= self.estimator_weights_.sum()\n",
    "        if n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            return pred.sum(axis=1)\n",
    "        return pred\n",
    "    \n",
    "    def staged_decision_function(self, X):\n",
    "        \"\"\"Compute decision function of ``X`` for each boosting iteration.\n",
    "        This method allows monitoring (i.e. determine error on testing set)\n",
    "        after each boosting iteration.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "        Returns\n",
    "        -------\n",
    "        score : generator of array, shape = [n_samples, k]\n",
    "            The decision function of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "            Binary classification is a special cases with ``k == 1``,\n",
    "            otherwise ``k==n_classes``. For binary classification,\n",
    "            values closer to -1 or 1 mean more like the first or second\n",
    "            class in ``classes_``, respectively.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"n_classes_\")\n",
    "        X = self._validate_X_predict(X)\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_[:, np.newaxis]\n",
    "        pred = None\n",
    "        norm = 0.\n",
    "\n",
    "        for weight, estimator in zip(self.estimator_weights_,\n",
    "                                     self.estimators_):\n",
    "            norm += weight\n",
    "\n",
    "            if self.algorithm == 'SAMME.R':\n",
    "                # The weights are all 1. for SAMME.R\n",
    "                current_pred = _samme_proba(estimator, n_classes, X)\n",
    "            else:  # elif self.algorithm == \"SAMME\":\n",
    "                current_pred = estimator.predict(X)\n",
    "                current_pred = (current_pred == classes).T * weight\n",
    "\n",
    "            if pred is None:\n",
    "                pred = current_pred\n",
    "            else:\n",
    "                pred += current_pred\n",
    "\n",
    "            if n_classes == 2:\n",
    "                tmp_pred = np.copy(pred)\n",
    "                tmp_pred[:, 0] *= -1\n",
    "                yield (tmp_pred / norm).sum(axis=1)\n",
    "            else:\n",
    "                yield pred / norm\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "        The predicted class probabilities of an input sample is computed as\n",
    "        the weighted mean predicted class probabilities of the classifiers\n",
    "        in the ensemble.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples]\n",
    "            The class probabilities of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"n_classes_\")\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "        X = self._validate_X_predict(X)\n",
    "\n",
    "        if n_classes == 1:\n",
    "            return np.ones((X.shape[0], 1))\n",
    "\n",
    "        if self.algorithm == 'SAMME.R':\n",
    "            # The weights are all 1. for SAMME.R\n",
    "            proba = sum(_samme_proba(estimator, n_classes, X)\n",
    "                        for estimator in self.estimators_)\n",
    "        else:   # self.algorithm == \"SAMME\"\n",
    "            proba = sum(estimator.predict_proba(X) * w\n",
    "                        for estimator, w in zip(self.estimators_,\n",
    "                                                self.estimator_weights_))\n",
    "\n",
    "        proba /= self.estimator_weights_.sum()\n",
    "        proba = np.exp((1. / (n_classes - 1)) * proba)\n",
    "        normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "        normalizer[normalizer == 0.0] = 1.0\n",
    "        proba /= normalizer\n",
    "\n",
    "        return proba\n",
    "\n",
    "    def staged_predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "        The predicted class probabilities of an input sample is computed as\n",
    "        the weighted mean predicted class probabilities of the classifiers\n",
    "        in the ensemble.\n",
    "        This generator method yields the ensemble predicted class probabilities\n",
    "        after each iteration of boosting and therefore allows monitoring, such\n",
    "        as to determine the predicted class probabilities on a test set after\n",
    "        each boost.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "        Returns\n",
    "        -------\n",
    "        p : generator of array, shape = [n_samples]\n",
    "            The class probabilities of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "        \"\"\"\n",
    "        X = self._validate_X_predict(X)\n",
    "\n",
    "        n_classes = self.n_classes_\n",
    "        proba = None\n",
    "        norm = 0.\n",
    "\n",
    "        for weight, estimator in zip(self.estimator_weights_,\n",
    "                                     self.estimators_):\n",
    "            norm += weight\n",
    "\n",
    "            if self.algorithm == 'SAMME.R':\n",
    "                # The weights are all 1. for SAMME.R\n",
    "                current_proba = _samme_proba(estimator, n_classes, X)\n",
    "            else:  # elif self.algorithm == \"SAMME\":\n",
    "                current_proba = estimator.predict_proba(X) * weight\n",
    "\n",
    "            if proba is None:\n",
    "                proba = current_proba\n",
    "            else:\n",
    "                proba += current_proba\n",
    "\n",
    "            real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))\n",
    "            normalizer = real_proba.sum(axis=1)[:, np.newaxis]\n",
    "            normalizer[normalizer == 0.0] = 1.0\n",
    "            real_proba /= normalizer\n",
    "\n",
    "            yield real_proba\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"Predict class log-probabilities for X.\n",
    "        The predicted class log-probabilities of an input sample is computed as\n",
    "        the weighted mean predicted class log-probabilities of the classifiers\n",
    "        in the ensemble.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples]\n",
    "            The class probabilities of the input samples. The order of\n",
    "            outputs is the same of that of the `classes_` attribute.\n",
    "        \"\"\"\n",
    "        return np.log(self.predict_proba(X))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for X.\n",
    "        The predicted class of an input sample is computed as the weighted mean\n",
    "        prediction of the classifiers in the ensemble.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        pred = self.decision_function(X)\n",
    "\n",
    "        if self.n_classes_ == 2:\n",
    "            return self.classes_.take(pred > 0, axis=0)\n",
    "\n",
    "        return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n",
    "\n",
    "    def staged_predict(self, X):\n",
    "        \"\"\"Return staged predictions for X.\n",
    "        The predicted class of an input sample is computed as the weighted mean\n",
    "        prediction of the classifiers in the ensemble.\n",
    "        This generator method yields the ensemble prediction after each\n",
    "        iteration of boosting and therefore allows monitoring, such as to\n",
    "        determine the prediction on a test set after each boost.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape = [n_samples, n_features]\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : generator of array, shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        n_classes = self.n_classes_\n",
    "        classes = self.classes_\n",
    "\n",
    "        if n_classes == 2:\n",
    "            for pred in self.staged_decision_function(X):\n",
    "                yield np.array(classes.take(pred > 0, axis=0))\n",
    "\n",
    "        else:\n",
    "            for pred in self.staged_decision_function(X):\n",
    "                yield np.array(classes.take(\n",
    "                    np.argmax(pred, axis=1), axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
